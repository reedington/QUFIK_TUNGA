{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5XEX3lGVES-"
      },
      "source": [
        "###*Installing & Importing our dependicies*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_fvvDrecHoZ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/deepseek-ai/DeepSeek-Coder.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd DeepSeek-Coder"
      ],
      "metadata": {
        "id": "prrUspTtt97f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "id": "fq2KZgwhT9YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q python-docx fpdf"
      ],
      "metadata": {
        "id": "jLOUJfnRSnd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Importing our depending used for our use case:*\n",
        "\n",
        "\n",
        "*   Article Generation\n",
        "*   Code Documentation and\n",
        "*   Error Detection and Fixing of errors\n",
        "\n"
      ],
      "metadata": {
        "id": "a4VYgSmfxXJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True)\n",
        "model = AutoModelForCausalLM. from_pretrained (\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True).cuda()\n"
      ],
      "metadata": {
        "id": "nHvEon4sZx40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Building our article generation*"
      ],
      "metadata": {
        "id": "KU3PX1bPyL6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install azure-storage-blob azure-identity azure-cognitiveservices-vision-computervision azure-cognitiveservices-speech\n"
      ],
      "metadata": {
        "id": "0FCW8rTdxgzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n",
        "!sudo dpkg -i libssl1.1_1.1.1f-1ubuntu2_amd64.deb"
      ],
      "metadata": {
        "id": "AhGgTNXn3SCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "import requests\n",
        "import uuid\n",
        "import json\n",
        "import azure.cognitiveservices.speech as speechsdk\n",
        "import sys\n",
        "\n",
        "\n",
        "def generate_article():\n",
        "    \"\"\"\n",
        "    Generates articles with titles based on user input using a machine learning model.\n",
        "\n",
        "    Returns:\n",
        "    - str: Generated article based on user input.\n",
        "\n",
        "    This function interacts with the user by accepting input through a chat-like interface ('User: ...').\n",
        "    It utilizes a machine learning model (represented by 'model' and 'tokenizer') to generate articles\n",
        "    based on the user's input. The 'max_length' parameter defines the maximum length of the generated article.\n",
        "    \"\"\"\n",
        "\n",
        "    user_input = input('User: ')  # Prompt user for input\n",
        "    language = input('What language do you want it in? ').lower()\n",
        "    languages = []\n",
        "    if language == 'english':\n",
        "        languages = ['en']\n",
        "    elif language == 'yoruba':\n",
        "        languages.append('yo')\n",
        "    elif language == 'igbo':\n",
        "        languages.append('ig')\n",
        "    elif language == 'hausa':\n",
        "        languages.append('ha')\n",
        "    else:\n",
        "        languages.append('en')\n",
        "    input_tokens = tokenizer(user_input, return_tensors='pt').to(model.device)\n",
        "\n",
        "    # Generating article based on user input\n",
        "    outputs = model.generate(\n",
        "        **input_tokens,\n",
        "        max_length=10678,\n",
        "        temperature=1,\n",
        "        top_p=0.7,\n",
        "        num_beams=6,\n",
        "        penalty_alpha=1,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True\n",
        "    )\n",
        "\n",
        "    # Decode the generated sequences to obtain the article\n",
        "    generated_article = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "    # Add your key and endpoint\n",
        "    key = \"2df6023bcb7445dd9ea3d38929bbb5e7\"\n",
        "    endpoint = \"https://api.cognitive.microsofttranslator.com\"\n",
        "    location = \"southafricanorth\"\n",
        "\n",
        "    path = '/translate'\n",
        "    constructed_url = endpoint + path\n",
        "\n",
        "    params = {\n",
        "        'api-version': '3.0',\n",
        "        'from': 'en',\n",
        "        'to': languages\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        'Ocp-Apim-Subscription-Key': key,\n",
        "        'Ocp-Apim-Subscription-Region': location,\n",
        "        'Content-type': 'application/json',\n",
        "        'X-ClientTraceId': str(uuid.uuid4())\n",
        "    }\n",
        "\n",
        "    body = [{\n",
        "        'text': generated_article\n",
        "    }]\n",
        "\n",
        "    request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
        "    response = request.json()\n",
        "\n",
        "    return json.dumps(response[0]['translations'][0]['text'], sort_keys=True, ensure_ascii=False, indent=4, separators=(',', ': '))\n",
        "\n",
        "def format_translated_article(translated_article):\n",
        "    translated_article = translated_article.replace('\\\\n', '\\n')\n",
        "    paragraphs = translated_article.split('\\n\\n')\n",
        "\n",
        "    formatted_article = \"\"\n",
        "    for paragraph in paragraphs:\n",
        "        if paragraph.startswith('**'):\n",
        "            formatted_article += f\"\\nâ€¢ {paragraph.replace('**', '')}\\n\"\n",
        "        else:\n",
        "            formatted_article += f\"\\n{paragraph}\\n\"\n",
        "\n",
        "    return formatted_article\n",
        "\n",
        "def speech_synthesis_to_mp3_file(formatted_article):\n",
        "    \"\"\"performs speech synthesis to a mp3 file\"\"\"\n",
        "    # Creates an instance of a speech config with specified subscription key and service region.\n",
        "    speech_config = speechsdk.SpeechConfig(subscription='23400053a3ea4672b9c1b0fb49ab7b62', region='southafricanorth')\n",
        "    # Sets the synthesis output format.\n",
        "    speech_config.set_speech_synthesis_output_format(speechsdk.SpeechSynthesisOutputFormat.Audio16Khz32KBitRateMonoMp3)\n",
        "    # Creates a speech synthesizer using file as audio output.\n",
        "    file_name = \"outputaudio.mp3\"\n",
        "    file_config = speechsdk.audio.AudioOutputConfig(filename=file_name)\n",
        "    speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=file_config)\n",
        "\n",
        "    # Receives a text from console input and synthesizes it to mp3 file.\n",
        "    while True:\n",
        "        print(\"Enter some text that you want to synthesize, Ctrl-Z to exit\")\n",
        "        try:\n",
        "            text = formatted_article\n",
        "        except EOFError:\n",
        "            break\n",
        "        result = speech_synthesizer.speak_text_async(text).get()\n",
        "        # Check result\n",
        "        if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
        "            print(\"Speech synthesized for text [{}], and the audio was saved to [{}]\".format(text, file_name))\n",
        "        elif result.reason == speechsdk.ResultReason.Canceled:\n",
        "            cancellation_details = result.cancellation_details\n",
        "            print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
        "            if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
        "                print(\"Error details: {}\".format(cancellation_details.error_details))\n",
        "\n",
        "def main():\n",
        "    generated_article = generate_article()\n",
        "    formatted_translated_article = format_translated_article(generated_article)\n",
        "    speech_synthesis_to_mp3_file(formatted_translated_article)\n",
        "\n",
        "    print(\"Formatted Translated Article:\")\n",
        "    print(formatted_translated_article)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "lXVJ_MJeziaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Generating Documentation and Comments for Articles:*\n",
        "*   This function aids in creating comprehensive documentation and comments for every line of code. Its purpose is to assist both current and new contributors to the codebase in comprehending legacy code. Moreover, it serves as a guide for identifying sections that might require updates or modifications."
      ],
      "metadata": {
        "id": "s2v2SyhWCm_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_code_documentation(code, language, max_length):\n",
        "    \"\"\"\n",
        "    Generates documentation and comments for the provided code snippet in the specified language.\n",
        "\n",
        "    Args:\n",
        "    - code (str): The code snippet to be documented.\n",
        "    - language (str): The programming language used in the code snippet.\n",
        "    - max_length (int): Maximum length of the generated documentation.\n",
        "\n",
        "    Returns:\n",
        "    - str: Generated documentation and comments for the code.\n",
        "\n",
        "    This function utilizes a machine learning model -(Deepseek-coder) (represented by 'model' and 'tokenizer')\n",
        "    to create comprehensive documentation and comments for the given code. It aims to clarify\n",
        "    each line's purpose and functionality for future developers working on the codebase.\n",
        "    Additionally, it takes into account the specified programming language for context.\n",
        "\n",
        "    If necessary, the function could help align variable names with the function's goal,\n",
        "    and it aims to improve the code's clarity and maintainability through better documentation.\n",
        "\n",
        "    Example Usage:\n",
        "    >>> generated_documentation = generate_code_documentation(code, 'Python', 500)\n",
        "    \"\"\"\n",
        "\n",
        "    input_text = f\"Please generate documentation and comments for the following {language.upper()} function/code. The goal is to clarify each line's purpose and functionality for future developers working on this codebase: {code}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generating documentation and comments\n",
        "    outputs = model.generate(**inputs,\n",
        "                             max_length=10678,\n",
        "                             temperature=0.8,\n",
        "                             top_p=0.7,\n",
        "                             num_beams=4,\n",
        "                             penalty_alpha=1,\n",
        "                             return_dict_in_generate=True,\n",
        "                             output_scores=True)\n",
        "\n",
        "    transition_scores = model.compute_transition_scores(\n",
        "        outputs.sequences,\n",
        "        outputs.scores,\n",
        "        outputs.beam_indices,\n",
        "        normalize_logits=False)\n",
        "\n",
        "    # Decoding generated sequences to obtain documentation\n",
        "    generated_documentation = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_documentation\n"
      ],
      "metadata": {
        "id": "o61uyeQ8GvJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_code_documentation(\"\"\"#include <iostream>\n",
        "\n",
        "unsigned long long factorial(unsigned int n) {\n",
        "    if (n == 0 || n == 1) {\n",
        "        return 1;\n",
        "    } else {\n",
        "        return n * factorial(n - 1);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    unsigned int number = 5;\n",
        "    unsigned long long result = factorial(number);\n",
        "    std::cout << \"Factorial of \" << number << \" is: \" << result << std::endl;\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "\"\"\",'c++',800)"
      ],
      "metadata": {
        "id": "oTR1zn3AIRqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Identifying bugs and providing solutions for them:*\n",
        "\\\n",
        "*   The primary objective of this function is to assist teams in enhancing their code quality by promptly detecting bugs and providing viable solutions to resolve them.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tau7NckZJw-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_and_fix_bugs(language, code_base):\n",
        "    \"\"\"\n",
        "    Uses a machine learning model-(Deepseek-coder) to detect bugs and suggest fixes or improvements for the given code.\n",
        "\n",
        "    Args:\n",
        "    - language (str): The programming language used in the code.\n",
        "    - code_base (str): The code snippet or codebase to be analyzed.\n",
        "\n",
        "    Returns:\n",
        "    - str: Generated suggestions for bug fixes or improvements.\n",
        "\n",
        "    This function takes the provided code snippet in the specified programming language and\n",
        "    utilizes a machine learning model (represented by 'model' and 'tokenizer') to identify\n",
        "    any bugs or potential issues. It then suggests fixes or improvements to enhance the code's quality.\n",
        "\n",
        "    Example Usage:\n",
        "    >>> suggested_fixes = detect_and_fix_bugs('Python', 'def calculate_average(numbers): ...')\n",
        "    \"\"\"\n",
        "\n",
        "    input_text = f'Please review the following {language} code and identify any bugs or potential issues. If you find any errors, please suggest a fix or improvements to the code: {code_base}'\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generating suggestions for bug fixes or improvements\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=3589,\n",
        "        temperature=0.8,\n",
        "        top_p=0.7,\n",
        "        num_beams=4,\n",
        "        penalty_alpha=1,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True\n",
        "    )\n",
        "\n",
        "    transition_scores = model.compute_transition_scores(\n",
        "        outputs.sequences,\n",
        "        outputs.scores,\n",
        "        outputs.beam_indices,\n",
        "        normalize_logits=False\n",
        "    )\n",
        "\n",
        "    # Decoding generated sequences to obtain suggestions\n",
        "    generated_suggestions = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_suggestions\n"
      ],
      "metadata": {
        "id": "8IdOyKNaL5YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detect_and_fix_bugs('python',\"\"\"def calculate_average(numbers):\n",
        "    total = 0\n",
        "    count = 0\n",
        "\n",
        "    for num in numbers:\n",
        "        total += num\n",
        "        count += 1\n",
        "\n",
        "    average = total / count  # Potential division by zero if 'count' is 0\n",
        "    return average\n",
        "\n",
        "# Test the function\n",
        "values = [10, 20, 30, 40, 50]\n",
        "result = calculate_average(values)\n",
        "print(\"The average is:\", result)\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "_YNH-ZrJP8Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2WLhKVgQdEaY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}